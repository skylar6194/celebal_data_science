{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Gagan's RAG Chatbot for Loan Approval Data**"
      ],
      "metadata": {
        "id": "ZSsDrUVRpBNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieval-Augmented Generation (RAG)** lets a chatbot answer questions by combining a language model with an **external knowledge base**.  \n",
        "\n",
        "In a RAG pipeline,the information retrieval step first finds relevant data from a corpus, and then a generative model produces a fluent answer using that data.  \n",
        "\n",
        "In our case, the “knowledge base” is a loan dataset (from Kaggle’s Loan Approval dataset), here we build a simple RAG system to answer questions about it."
      ],
      "metadata": {
        "id": "g-gqB-swpLAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading and EDA-ying the data**"
      ],
      "metadata": {
        "id": "mP7nM3WJphF7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnKwSk_WnblC",
        "outputId": "5c04132d-9d2f-4456-cd9e-e9d2f64660e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "(614, 13)\n",
            "    Loan_ID Gender Married Dependents Education Self_Employed  \\\n",
            "0  LP001002   Male      No          0  Graduate            No   \n",
            "1  LP001003   Male     Yes          1  Graduate            No   \n",
            "2  LP001005   Male     Yes          0  Graduate           Yes   \n",
            "\n",
            "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
            "0             5849                0.0         NaN             360.0   \n",
            "1             4583             1508.0       128.0             360.0   \n",
            "2             3000                0.0        66.0             360.0   \n",
            "\n",
            "   Credit_History Property_Area Loan_Status  \n",
            "0             1.0         Urban           Y  \n",
            "1             1.0         Rural           N  \n",
            "2             1.0         Urban           Y  \n"
          ]
        }
      ],
      "source": [
        "!pip install pandas scikit-learn transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/content/Training Dataset.csv')\n",
        "print(df.shape)\n",
        "print(df.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row corresponds to one loan application, with features such as Loan_ID, Gender, Married, ApplicantIncome, LoanAmount, Credit_History, Property_Area, and the target Loan_Status (Y / N)."
      ],
      "metadata": {
        "id": "ACcQYbFrp51R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will treat each row as a “document” by concatenating its fields into a text string.\n",
        "\n",
        "We store all such documents in a list."
      ],
      "metadata": {
        "id": "N_eQoFVSqGh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "for _, row in df.iterrows():\n",
        "    #handle missing values by converting to string\n",
        "    vals = row.fillna('').astype(str)\n",
        "    text = (\n",
        "        f\"Loan_ID: {vals['Loan_ID']}, Gender: {vals['Gender']}, Married: {vals['Married']}, \"\n",
        "        f\"Dependents: {vals.get('Dependents', '')}, Education: {vals['Education']}, Self_Employed: {vals['Self_Employed']}, \"\n",
        "        f\"ApplicantIncome: {vals['ApplicantIncome']}, CoapplicantIncome: {vals['CoapplicantIncome']}, \"\n",
        "        f\"LoanAmount: {vals['LoanAmount']}, Loan_Amount_Term: {vals['Loan_Amount_Term']}, \"\n",
        "        f\"Credit_History: {vals['Credit_History']}, Property_Area: {vals['Property_Area']}, Loan_Status: {vals['Loan_Status']}\"\n",
        "    )\n",
        "    docs.append(text)\n",
        "\n",
        "#example doc string\n",
        "print(docs[0][:100], \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvFvisyVqCyF",
        "outputId": "d549547b-df34-4923-e153-a09fc0580824"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loan_ID: LP001002, Gender: Male, Married: No, Dependents: 0, Education: Graduate, Self_Employed: No, ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have docs, a list of strings where each string encodes one row’s information.  \n",
        "\n",
        "These will serve as “knowledge documents” for retrieval."
      ],
      "metadata": {
        "id": "wM-qkoYDqX1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Simple TF IDF retriver**"
      ],
      "metadata": {
        "id": "qE9yqsQWqddo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used a TF-IDF vectorizer to turn each document into a vector.  \n",
        "\n",
        "This allows us to compute similarity between a user’s query and each row.\n",
        "\n",
        "We choose TF-IDF for simplicity; in practice one could also use semantic embeddings. Here, to ignore common English stop words."
      ],
      "metadata": {
        "id": "DpVheKEwql9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#TF IDF matrix for documents\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "doc_vectors = vectorizer.fit_transform(docs)   #shape: (#docs, vocab_size)"
      ],
      "metadata": {
        "id": "1qK4MIFwqvD1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defined a function to retrieve the top-K most relevant documents given a text query.  \n",
        "\n",
        "It vectorizes the query and computes cosine similarity with all row vectors, then returns the top matches."
      ],
      "metadata": {
        "id": "p6DjkJQeq41D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_top_docs(query, top_k=3):\n",
        "\n",
        "    q_vec = vectorizer.transform([query])\n",
        "    #computes cosine similarity with all documents\n",
        "    scores = cosine_similarity(q_vec, doc_vectors).flatten()\n",
        "    #gets indices of top k scores\n",
        "    top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "    #return the indiced docs\n",
        "    return [docs[i] for i in top_indices]\n",
        "\n",
        "#example retrieval test\n",
        "query = \"high income and good credit history\"\n",
        "top_docs = retrieve_top_docs(query, top_k=2)\n",
        "print(\"Top retrieved rows for query:\", query)\n",
        "for doc in top_docs:\n",
        "    print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1wJhEq4q_ue",
        "outputId": "d500f29f-fa89-4f0c-9271-86a83f0a14e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top retrieved rows for query: high income and good credit history\n",
            "Loan_ID: LP001002, Gender: Male, Married: No, Dependents: 0, Education: Graduate, Self_Employed: No, ApplicantIncome: 5849, CoapplicantIncome: 0.0, LoanAmount: , Loan_Amount_Term: 360.0, Credit_History: 1.0, Property_Area: Urban, Loan_Status: Y\n",
            "Loan_ID: LP002990, Gender: Female, Married: No, Dependents: 0, Education: Graduate, Self_Employed: Yes, ApplicantIncome: 4583, CoapplicantIncome: 0.0, LoanAmount: 133.0, Loan_Amount_Term: 360.0, Credit_History: 0.0, Property_Area: Semiurban, Loan_Status: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generating Answers with an LLM**"
      ],
      "metadata": {
        "id": "V02-B_CTrbMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a **pretrained lightweight generative** model to produce answers. We’ll use **Hugging Face’s pipeline with a small T5 model (google/flan-t5-small)** for text generation.\n",
        "\n",
        "This runs locally and is free to use. In practice, larger models or APIs (GPT-3.5, Llama2, etc.) could improve quality, but FLAN-T5-small suffices for a demo."
      ],
      "metadata": {
        "id": "jzgdlew5riQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "#Init the text-to-text generation pipeline\n",
        "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
        "\n",
        "#example: Ask a question and generate an answer\n",
        "user_query = \"Which applicants have loans approved?\"\n",
        "retrieved = retrieve_top_docs(user_query, top_k=3)\n",
        "prompt = (\n",
        "    \"Based on the following loan records:\\n\" +\n",
        "    \"\\n\".join(retrieved) +\n",
        "    \"\\nAnswer the question: \" + user_query\n",
        ")\n",
        "result = generator(prompt, max_length=100)[0]['generated_text']\n",
        "\n",
        "print(\"Generated Answer:\", result)"
      ],
      "metadata": {
        "id": "1XIvMsnDryg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code feeds the retrieved context and question to the model.  \n",
        "The LLM then outputs a natural-sounding answer.  \n",
        "\n",
        "For instance, it might say something like “According to the data above, loans are approved for applicants with [some pattern]...”, thereby “referencing” the dataset content.  \n",
        "\n",
        "(The actual output depends on the model; FLAN may give a generic answer. In a real chatbot, one could further prompt the model to explicitly cite the loan IDs or features to increase trust.)"
      ],
      "metadata": {
        "id": "RQUIQrbZsQb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example Queries**"
      ],
      "metadata": {
        "id": "HKrmC1upsaqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample q&a\n",
        "queries = [\n",
        "    \"Is a married applicant more likely to get a loan?\",\n",
        "    \"What was the loan amount of the applicant with the highest income?\",\n",
        "]\n",
        "for q in queries:\n",
        "    top_rows = retrieve_top_docs(q, top_k=3)\n",
        "    prompt = (\n",
        "        \"Loan records:\\n\" + \"\\n\".join(top_rows) +\n",
        "        f\"\\nQuestion: {q}\\nAnswer:\"\n",
        "    )\n",
        "    ans = generator(prompt, max_length=80)[0]['generated_text']\n",
        "    print(f\"\\nQ: {q}\\nA: {ans}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYDdqX_Hsdoi",
        "outputId": "91fe8b8f-dd13-43e9-b95d-4cdef1267455"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: Is a married applicant more likely to get a loan?\n",
            "A: Dependents: 0, Education: Not Graduate, Self_Employed: No, ApplicantIncome: 2000, CoapplicantIncome: 0.0, Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: , Loan_Amount: ,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What was the loan amount of the applicant with the highest income?\n",
            "A: 133.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The answers generated depend on how well the model uses the retrieved context. In practice, one might add instructions like “Answer with reference to the data above.” The key idea is that by augmenting the prompt with actual rows from the dataset, the model’s response is grounded in that data\n",
        "\n",
        "Summary: This notebook demonstrated a basic RAG pipeline:\n",
        "  1. treating each table row as a document, built a TF-IDF index for retrieval  \n",
        "  2. used a generative LLM to answer questions using the retrieved rows as context  \n",
        "  \n",
        "  This allows the chatbot to produce natural answers grounded in the loan dataset, improving factual accuracy and enabling reference to specific data entries"
      ],
      "metadata": {
        "id": "WqpBSF5Esm8_"
      }
    }
  ]
}